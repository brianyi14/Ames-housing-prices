---
title: "Ames Housing Prices"
author: Brian Yi, Alexa Edwards, Erica Chen, Minerva Fang
output: 
  github_document:
    default
  html_document:
    toc: true
    df_print: paged
---

```{r include=FALSE}
library(readr)
library(dplyr)
library(car)
library(leaps)
library(Stat2Data)
Ames <- read_csv("AmesTrain21.csv")
AmesTest <- read_csv("AmesTest21.csv")
```


# Introduction


**Purpose:** The purpose of this project is to develop and validate a model that can assist with prospective house owners in estimating house prices depending on the qualities they are looking for in their future home.

**Method of Approach:** This group project focuses on building and testing a multivariate linear regression model from the Ames Housing dataset. Our dataset is split between a training set that is used to "train" our model, and a testing set that we "test" our model on. First, we use best subsets regression, backward elimination, forward selection, and step-wise regression to build our initial model. We conduct some individual t-tests for slope to measure the significance of the predictors in our model. We also exmaine the variance inflation factor (VIF) to detect any multicollinearity between our predictors. Next, we do some residual analysis through residual vs fits plots, histogram distributions, and normal quantile plots. We also compute standarized residuals to pinpoint potential outliers. 

Our initial model has some insignificant variables so we transform these predictors so that they have a stronger correlation with our response variable, `Price`. With this transformed model, we conduct the same hypothesis tests and residual analysis that we did for our initial model. Finally, we cross-validate our transformed model with the testing set to evaluate its effectiveness in predicting house prices. To finish our report, we take our final model out for a spin by predicting a particular housing price to see if the results are reasonable.

**Results:** Our final transformed model fits our data well and fulfills all the metrics that we evaluated it with. There are a couple of things about how our methodology can be potentially lacking that will be addressed thoroughly in the conclusion. 


# Build an Initial Model


## Best Subsets Regression


We will first use best subsets regression to find our initial model. This method investigates the adjusted R-squared and Mallows' Cp values of different models created with various subsets of predictors.
```{r results = "hide"}
# Model with all the possible predictors
Full = lm(formula = Price ~ LotFrontage + LotArea + Quality + Condition + YearBuilt + YearRemodel + BasementSF + GroundSF + BasementFBath + BasementHBath + FullBath + HalfBath + Bedroom + TotalRooms + Fireplaces + GarageCars + GarageSF + WoodDeckSF + OpenPorchSF + EnclosedPorchSF + ScreenPorchSF, data = Ames)

# Finding all possible subsets/models
all = regsubsets(Price ~ LotFrontage + LotArea + Quality + Condition + YearBuilt + YearRemodel + BasementSF + GroundSF + BasementFBath + BasementHBath + FullBath + HalfBath + Bedroom + TotalRooms + Fireplaces + GarageCars + GarageSF + WoodDeckSF + OpenPorchSF + EnclosedPorchSF + ScreenPorchSF, data = Ames, nbest = 2, nvmax = 30)

# Method for visualizing our results
ShowSubsets = function(regout){
  z = summary(regout)
  q = as.data.frame(z$outmat)
  q$Rsq = round(z$rsq * 100, 2)
  q$adjRsq = round(z$adjr2 * 100, 2)
  q$Cp = round(z$cp, 2)
  return(q)
}

ShowSubsets(all)
```

The higher the R-squared value of a linear model, the better that model fits the data. Since we are conducting a multivariate analysis, we will look at adjusted R-squared that also accounts for adding predictors that don't improve our model. There are a few models, 15(1), 16(1), 16(2), 17(1), 17(2), 18(1), with the highest adjusted R-squared value of 85.19 so we look at another metric, Mallow's Cp, to help shed some light on the predictors not in the model. As a general rule, a smaller Mallow's Cp indicates a better model. Model 15(1) has the smallest Cp of 13.31 while still having the higest adjusted R-squared value of 85.19, indicating that it is the best model by this method. However, this does not rule out the other models listed because their Mallows' Cp values are all less than k + 1 (k being the number of predictors); in other words, the other models I mentioned above also have very little to no bias in predicted responses.

Our best model based off of R-squared and Mallow's Cp is the following:
```{r}
# Best model from best subsets regression
modAmes = lm(formula = Price ~ LotFrontage + LotArea + Quality + Condition + YearBuilt + 
    YearRemodel + BasementSF + GroundSF + BasementFBath + FullBath + 
    Bedroom + TotalRooms + Fireplaces + GarageSF + ScreenPorchSF, data = Ames)
```

We now run backward elimination, forward selection, and step-wise regression to determine whether the initial model we selected is the best fit for our data.


## Backward Elimination


We conduct backward elimination first because it requires fitting fewer models but still leaves us with only significant predictors. Backward elimination starts with the full model (with all the predictors) and then takes out any predictor with a p-value above our 5% criterion individually until we reach our desired model.
```{r}
# Backward elimination
MSE = (summary(Full)$sigma)^2
step(Full, scale = MSE)
```

Turns out, backward elimination also yields the model with the same fifteen predictors as the one discovered from best subsets regression!


## Forward Selection


Backward elimination has numerous drawbacks such as eliminating a variable early on that might have significance later on. Therefore, we decide to use forward selection that starts with the single best predictor and then adds onto it until there are no more significant predictors to add to our model. *We don't show the output of forward selection for length purposes of this report.*
```{r results = "hide"}
# Forward selection
MSE = (summary(Full)$sigma)^2
none = lm(Price ~ 1, data = Ames)
step(none, scope = list(upper = Full), scale = MSE, direction = "forward")
```

Forward selection also chooses the model with the same fifteen variables that we have in the previously built models.


## Step-wise Regression


Forward selection also has its disadvantages since a predictor selected earlier might become insignificant later on and only serve to crowd the model. Therefore, we decide to use step-wise regression to cover for the disadvantages seen in backward elimination and forward selection. *Again, we don't show the output of step-wise selection for length purposes of this report.*
```{r results = "hide"}
# Step-wise Regression
MSE = (summary(Full)$sigma)^2
none = lm(Price ~ 1, data = Ames)
step(none, scope = list(upper = Full), scale = MSE)
```

Although a bit tedious, step-wise regression also yields the same fifteen predictor model we found earlier. Since all four methods fitted the same fifteen-predictor model, we decide to settle with it as our initial model and move on to evaluating it.


## Scatterplots


We first plot the individual scatterplots between the fifteen predictors and `price`.

```{r}
# Scatterplots of each predictor with price
plot(Price ~ LotFrontage + LotArea + Quality + Condition + YearBuilt + 
    YearRemodel + BasementSF + GroundSF + BasementFBath + FullBath + 
    Bedroom + TotalRooms + Fireplaces + GarageSF + ScreenPorchSF, data = Ames)
```

When it comes to parts of a house, I feel that the more you have, whether it is the number of rooms or square footage of the ground floor, the more desirable it becomes for the owner. The scatterplots all follow this assumption in that as the individual predictors increase in amount, so does `price`. For example, let us take a look at `GroundSF`. As `GroundSF` increases, `price` also increases since the amount of square feet of a house is probably a factor many people value in their house. We won't analyze the plots one by one here, but later on we will transform our model using these individual scatterplots.


## Predictor Analysis


Next we investigate to see if any of the predictors in our model are not significant at the 5% level through individual t-tests.
```{r}
summary(modAmes)
```

The `FullBath` predictor is the only predictor in the model that is not significant. Even though the slope of `FullBath` is not significant in this model, it has meaningful correlation with the other variables such that without it, the model would suffer in its capability to predict `price`.


## Multicollinearity


We look at variance inflation factors (VIF) to detect multicollinearity between predictors. Multicollinearity is when predictors have a correlation with each other that may have a negative effect on your model.
```{r}
vif(modAmes)
```

In general, VIF values close to 1 indicate that the predictors don't have much correlation, VIF of 1-5 indicates moderate correlation, and VIF >5 indicates high correlation. All VIF values are less than 5 with the exception of `GroundSF`, which has a VIF of 5.031492. Thus, `GroundSF` exhibits multicollinearity with respect to the other predictors in the model and is a variable we should transform or consider removing.


# Residual Analysis for Initial Model


We do some residual analysis to check for some basic assumptions of our linear regression model. These basic conditions include linearity, constant variance, and normality.


## Residuals vs Fits Plot

We start off with a Residuals vs Fits Plot to assess the linearity and variance of our model.
```{r}
plot(modAmes$residuals~modAmes$fitted.values, data = Ames)
abline(0,0)
```

The residuals are centered around zero overall, which is what we want to see in a residuals vs fits plot. However, there is a distinct curvature to our plot indicating that linearity does not hold too well. The curved pattern is also responsible for the non-constant variance since the variance increases at the left and right sides of this plot.


## Normal Quantile Plot and Histogram


We look at a normal quantile plot and a residual histogram to evaluate the normality of our model.
```{r}
# Normal quantile plot
qqnorm(modAmes$residuals)
qqline(modAmes$residuals)

hist(modAmes$residuals)
```

If the residuals are normally distributed, our data will form a diagonal line in the normal quantile plot. In we look at the plot, most of the data sticks well to the line. However, there are a few points that are off the line at the left of the model, and a noticeable amount that is skewed at the upper right portion of the model, making us slightly concerned about the normality of the model. 

The histogram of the residuals again show that most of the residuals is centered around zero, backing up our residual vs fits plot analysis. Furthermore, the histogram shows that the residuals are normally distributed with a slight right skew, wich correlates with what we saw in our normal quantile plot.


## Standarized Residuals


Our last step in residual analysis is looking at standarized residuals to detect any outliers in our dataset.
```{r}
# Creating dataframe of standarized residuals
standard = data.frame(abs(rstandard(modAmes)))

# Picking the ten greatest residuals
standard %>%
  arrange(desc(abs.rstandard.modAmes..)) %>% 
  head(10)
```

Standardized residual values that are greater than 3 are considered extreme and potential outliers. Therefore, we look for any observations in our dataset that have an absolute residual value greater than 3. We see 9 values that are greater than 3 that can be considered as outliers in our data. One way to improve our model is to remove these outliers; we leave them in for now since these 9 points are less than 2% of our training dataset, and we also plan on transforming our model which may eliminate some of these outliers.


# Building a Better Model


## Transforming our Initial Model

Even though our initial model has decent normality and variance, it is somewhat lacking in linearity. We now want to fix some of these aspects of our initial model by transforming some of the predictors.
```{r}
# Transformed model
modAmes2 = lm(log(Price) ~ log(LotFrontage + LotArea) + I(Quality^2 + Condition) + I(YearBuilt + .5 * YearRemodel) + (BasementSF) + (GroundSF) + I(.5 * BasementFBath + .5 * FullBath) + I(Bedroom + TotalRooms) + Fireplaces + I(GarageSF + ScreenPorchSF), data = Ames)
```

We grouped variables together that we felt had some correlation with one another. For example, `Bedroom` and `TotalRooms` may be valued in a similar fashion between prospective buyers since more rooms are always better. We also made logistic and linear transformations that improved the linearity relationships seen in the individual scatterplots we plotted earlier. Next, we conduct similar model evaluation and residual analysis we did for our initial model.


## Evaluating Transformed Model


We start off our analysis of our transformed model by looking at the adjusted R-squared value as well as the residual standard error. Once again, we also want to see which of our predictors are significant through individual t-tests.
```{r}
summary(modAmes2)
```

Our new model has a higher adjusted R-squared value of 0.8562 (old model: 0.8519). However, our residual standard error is 0.1568, which is way too close to 0, making us think that our model may be overfitting the training set. We will find out whether this is the case when we look at the testing set. After various transformations, we are happy to see that all of the predictors are within the 5% significance interval.


## Residual vs Fits Plot


```{r}
plot(modAmes2$residuals~modAmes$fitted.values, data = Ames)
abline(0,0)
```

We see that the residual vs fits plot has a much more even distribution above and below the line throughout the graph. The distinct curved pattern we saw in our initial model has been replaced by a more random distribution throughout the plot, indicating that our transformed model is a much better fit for predicting prices from our data and that linearity holds. Furthermore, variance seems constant throughout.


## Normal Quantile Plot and Histogram


```{r}
# Normal quantile plot
qqnorm(modAmes2$residuals)
qqline(modAmes2$residuals)

hist(modAmes2$residuals)
```

The normal quantile plot shows that most of the points throughout the data sticks better to the line as compared to before. The left and right tails show less of a skew as compared to before, and this is matched in our histogram, which shows a normal distribution. Altogether, this is indicative that our transformed model preserves normality better than our initial model. This analysis is not surprising considering that our standard residual error was so close to zero.


## Standarized Residuals


```{r}
# Creating dataframe of standarized residuals
standard = data.frame(abs(rstandard(modAmes2)))

# Picking the ten greatest residuals
standard %>%
  arrange(desc(abs.rstandard.modAmes2..)) %>% 
  head(10)
```

This time we see only 3 values that are strictly greater than 3 that can be considered as outliers in our data. This is a great improvement when compared to the 9 outliers that we discovered previously.


## Cross-validation


We now take our final model we developed from the training set and cross-validate it with our testing set. It is important to know whether our model is a good fit for our data or if there is over/underfitting present.

```{r}
# Calculating shrinkage
fitPrice = predict(modAmes2, newdata = AmesTest)
crosscorr = cor(AmesTest$Price, fitPrice)
testingRsq = crosscorr^2
trainingRsq = .8583
trainingRsq - trainingRsq
```

We cross-validate our model by comparing the R-squared values of our model between the training and test set. We calculate the difference between the two R-squared values, otherwise known as shrinkage. For a model to be a good fit, the shrinkage must be close to 0. The shrinkage is -0.02312466, meaning that our testing data fits our model approximately 2% better than our training data. It happened just by chance that our testing set is a stronger fit for our model.


## Predicting a House Price


We now use our final model to predict the price of a house with the following characteristics.
```{r}
# House price we are trying to predict
newx=data.frame(TotalRooms = 9, YearBuilt = 1995, YearRemodel = 2003, LotArea = 11060, LotFrontage = 90, Quality = 7, Condition = 5,BasementSF = 1150, BasementFBath = 0, GroundSF = 2314, Bedroom = 3, FullBath = 2, Fireplaces = 1, GarageSF = 502, ScreenPorchSF = 0)

# Prediction interval
predict.lm(modAmes2, newx, interval="prediction", level = .95)
```

```{r}
# Our model logistically transforms price so we have to recalculate the prediction interval accordingly
exp(c(5.530761, 5.221046, 5.840476))
```

When buying a house that fits the relevant characteristics listed above, we are 95% confident that the price of the house will be between 185.1277 thousand dollars and 343.9430 thousand dollars. This doesn't seem unreasonable by just looking at the overall properties of this house.


# Conclusion


Our initial goal of predicting the price of a house by using a linear regression model is complete. Since our final model fulfills all the assumptions of linear regression, we can say that our model predicts housing prices pretty well. We should note that this dataset is from houses in Ames, Iowa and may not be indicative of housing prices elsewhere. We should also keep in mind that the total dataset is only 800 observations, which is not a lot. Furthermore, our cross-validation methodology involves dividing our dataset into training and testing sets before import, so it could be due to chance that our model fits the testing set so well. If we want to improve our model, we would want to conduct a k-fold cross-validation to reduce the chances of this possibility.