---
title: "Ames Housing Prices"
author: Brian Yi, Alexa Edwards, Erica Chen, Minerva Fang
output: 
  github_document:
    default
  html_document:
    toc: true
    df_print: paged
---

```{r include=FALSE}
library(readr)
library(dplyr)
library(car)
library(leaps)
library(Stat2Data)
Ames <- read_csv("AmesTrain21.csv")
AmesTest <- read_csv("AmesTest21.csv")
```

# Introduction

This project focuses on building a model for the Ames Housing dataset to predict housing prices. We first select a model through backward elimination, forward selection, and stepwise regression. Then we conduct residual analysis for this model to identify outliers. We then take our model out for a run to predict a housing price to see if the results are reasonable.

Next, we transform our model such that predictors will have a stronger correlation with our dependent variable, price. We finally conduct some cross-validation on our non-transformed model to see if our model does well in predicting on a test dataset.

# Part 1. Build an initial basic model

### Mallow's Cp and adjusted R-squared

We decide to use four methods to determine which model is best suited for predicting the price of a house in Ames, Iowa. Our first method investigates the adjusted R-squared values and Mallow's Cp for the best subsets. 
```{r}
# Model with all the variables before selection
Full = lm(formula = Price ~ LotFrontage + LotArea + Quality + Condition + YearBuilt + YearRemodel + BasementSF + GroundSF + BasementFBath + BasementHBath + FullBath + HalfBath + Bedroom + TotalRooms + Fireplaces + GarageCars + GarageSF + WoodDeckSF + OpenPorchSF + EnclosedPorchSF + ScreenPorchSF, data = Ames)

# Mallow's Cp and Adjusted R-squared selection
all = regsubsets(Price ~ LotFrontage + LotArea + Quality + Condition + YearBuilt + YearRemodel + BasementSF + GroundSF + BasementFBath + BasementHBath + FullBath + HalfBath + Bedroom + TotalRooms + Fireplaces + GarageCars + GarageSF + WoodDeckSF + OpenPorchSF + EnclosedPorchSF + ScreenPorchSF, data = Ames, nbest = 2, nvmax = 30)

# Visualization of our method
ShowSubsets=function(regout){
  z=summary(regout)
  q=as.data.frame(z$outmat)
  q$Rsq=round(z$rsq*100,2)
  q$adjRsq=round(z$adjr2*100,2)
  q$Cp=round(z$cp,2)
  return(q)
}

ShowSubsets(all)
```

The highest R-squared value is generally desired and we see that several models have the highest R-squared value of 85.19. However, the R-squared values depend only on the predictors in the model so looking at Mallow's Cp will shed light on the impact of predictors not in the model. As a general rule, a smaller Mallow's Cp is preferred. Model 15(1) has the lowest Cp of 13.31, while still having the higest R-squared value of 85.19, indicating that it is the best subset by this method.

Our best model based off of R-squared and Mallow's Cp is the following:
```{r}
# Best model from our first method
modAmes = lm(formula = Price ~ LotFrontage + LotArea + Quality + Condition + YearBuilt + 
    YearRemodel + BasementSF + GroundSF + BasementFBath + FullBath + 
    Bedroom + TotalRooms + Fireplaces + GarageSF + ScreenPorchSF, data = Ames)
```

We now run backward elimination, forward selection, and stepwise regression to see what other models are good.

## Backward Elimination

The next method we used is backward elimination because it requires fitting fewer models but still leaves us with only significant predictors. Backward elimination takes out any predictor that has a P-value above 5% step by step until we reach our desired model. 
```{r}
# Backward elimination
MSE = (summary(Full)$sigma)^2
step(Full, scale = MSE)
```

Turns out, backward elimination also yields the model with the same fifteen variables when compared with the model found from the first method.


## Forward Selection

Backward elimination has numerous drawbacks such as eliminating a variable early on that might have significance later on. Therefore the last method we decided to use is forward selection, which works from the other direction. We start with the single best predictor and then add onto it until there are no more significant predictors to add to our model. 
```{r include=FALSE}
# Forward selection
MSE = (summary(Full)$sigma)^2
none = lm(Price ~ 1, data = Ames)
step(none, scope = list(upper = Full), scale = MSE, direction = "forward")
```

Forward selection also chooses the model with the same fifteen variables when compared with the models found we found above.


## Stepwise Regression

Forward selection also has its disadvantages since a predictor selected earlier might become insignificant later on and only serve to crowd the model. Therefore, we decided to use stepwise regression to cover for the disadvantages of both backward elimination and forward selection.
```{r include=FALSE}
# Stepwise Regression
MSE = (summary(Full)$sigma)^2
none = lm(Price ~ 1, data = Ames)
step(none, scope = list(upper = Full), scale = MSE)
```

Although a bit tedious, stepwise regression also yielded the same fifteen predictor model we found earlier. Since all four methods yielded the same fifteen-predictor model, we decide to settle with this as our basic model and move on to its analysis.

## Scatterplots

We plot our final model to see any trends.

```{r}
# Scatterplots of each predictor with price
plot(Price ~ LotFrontage + LotArea + Quality + Condition + YearBuilt + 
    YearRemodel + BasementSF + GroundSF + BasementFBath + FullBath + 
    Bedroom + TotalRooms + Fireplaces + GarageSF + ScreenPorchSF, data = Ames)
```

We won't analyze the scatterplots one by one, but later on we will transform our model using these individual scatterplots.

## Predictor Analysis

Next we investigate to see if any of the predictors are not significant at the 5% level through t-tests for correlation.
```{r}
# Summary method to view predictor significance
summary(modAmes)
```

The FullBath predictor is the only predictor in the model that is not significant at a 5% level. Alone, the variable is not a good predictor of price; however, in combination with the other variables, it has meaningful correlation to the response variable.


## Multi-collinearity

We look at VIF values to see if any predictors have some correlation with each other.
```{r}
# VIF values
vif(modAmes)
```

All variable VIF values are less than 5 with the exception of GroundSF which has a VIF value of 5.031492. This value is greater than 5 which indicates that the variable shows multicollinearity in relation to the other predictors in the model. 


# Part 2. Residual analysis for our basic model

We do some residual analysis to check for basic conditions of our basic model.

## Residuals vs fits plot

```{r}
# Residuals vs fits plot
plot(modAmes$residuals~modAmes$fitted.values, data = Ames)
abline(0,0)
```

Regarding the conditions for a simple linear model,  In terms of error, the residuals are relatively centered around zero; however, they show a distinct curvature meaning that there is lack of independence in the data points that could benefit from some transformation. The residuals do not show a great amount of variance aside from the few outliers.

## Normal quantile plot and histogram

```{r}
# Histogram
hist(modAmes$residuals)

# Normal quantile plot
qqnorm(modAmes$residuals)
qqline(modAmes$residuals)
```

The histogram of the residuals show that the normality of the model is pretty good although there is a slight right skew. The normality is explored further in the normal qunatile plot that shows most of the points throughout the data sticks well to the line overall. However, there are a few data points that are off the line at the left of the model, and a sizeable amount that is skewed at the upper right portion of the model, indicating decent concern for the normality of the model.

## Standarized and studentized residuals

```{r}
# Creating dataframe
standard = data.frame(abs(rstandard(modAmes)))

# Standard residuals
standard2 <- standard %>%
  arrange(desc(abs.rstandard.modAmes..))
head(standard2,10)

# Studentized residuals
student = data.frame(abs(rstudent(modAmes)))
student2 <- student %>%
  arrange(desc(abs.rstudent.modAmes..))

head(student2,10)
```

Studentized and standardized residual values that are greater than 3 are considered to be outliers, therefore, we test for any values in the model that have absolute residual values that are greater than 3. By creating a dataframe and arranging the studentized and standardized absolute residual values from highest to lowest, we see 9 values that are greater than 3 that can be considered as outliers that are influential to the data.


# Part 3. Find a a fancier model

## Transformation

We now want to fix some of the issues with our basic model by building a fancier model through transforming our variables.
```{r}
# Transformed model
modAmes2 = lm(log(Price) ~ log(LotFrontage + LotArea) + I(Quality^2 + Condition) + I(YearBuilt + .5*YearRemodel) + (BasementSF) + (GroundSF) + I(.5*BasementFBath + .5*FullBath) + I(Bedroom + TotalRooms) + Fireplaces + I(GarageSF + ScreenPorchSF), data = Ames)
```

We grouped variables together that we felt had correlation to one another. For example, `Bedroom` and `TotalRooms` are valued equally between prospective buyers where more rooms are better. Our transformed model also has logistic and linear transformations based on the individual scatterplots we plotted from earlier.

## Predictor analysis

We look at the t-test for correlation to determine predictor significance for our transformed model.
```{r}
# Summary of fancier model
summary(modAmes2)
```

After various transformations, we notice that all of the predictors are within the 5% significance interval. We do a more in-depth analysis of our fancier model in the next part.


# Part 4. Residual analysis for your fancier model

## Residual vs fits plot

```{r}
# Residual vs fits plot
plot(modAmes2$residuals~modAmes$fitted.values, data = Ames)
abline(0,0)
```

We see that the residual vs fitted values plot for our fancier model has a much more even distribution above and below the line throughout the graph. The distinct curved pattern we saw in our basic model has been replaced by a more random distribution throughout the plot, indicating that our transformed model is a much better fit for predicting prices from our data.

## Normal quantile plot and histogram
```{r}
# Histogram
hist(modAmes2$residuals)

# Normal quantile plot
qqnorm(modAmes2$residuals)
qqline(modAmes2$residuals)
```

Although the histogram isn't centered properly, we see that the normality of the model is pretty good since the data itself is centered properly. The normality is explored further in the normal qunatile plot that shows most of the points throughout the data sticks well to the line overall. The left and right tails that are slightly skewed shows less of a skew when compared to before, indicating that our transformed model also preserves normality better than our basic model.

## Standarized and studentized Residual
```{r}
# Standardized residual
standard3 = data.frame(abs(rstandard(modAmes2)))
standard4 <- standard3 %>%
  arrange(desc(abs.rstandard.modAmes2..))
head(standard4,10)

# Studentized residual
student3 = data.frame(abs(rstudent(modAmes2)))
student4 <- student3 %>%
  arrange(desc(abs.rstudent.modAmes2..))

head(student4,10)
```

To revisit, studentized and standardized residual values that are greater than 3 are considered to be outliers, therefore, we test for any values in the model that have absolute residual values that are greater than 3. By creating a dataframe and arranging the studentized and standardized absolute residual values from highest to lowest, this time we see only 3 values that are strictly greater than 3 that can be considered as outliers that are influential to the data. This is a great improvement as compared to our 9 outliers that we discovered in our basic model.


# Part 5. Final model

We now use our final model to predict the price of a house with the following characteristics.
```{r}
newx=data.frame(TotalRooms = 9, YearBuilt = 1995, YearRemodel = 2003, LotArea = 11060, LotFrontage = 90, Quality = 7, Condition = 5,BasementSF = 1150, BasementFBath = 0, GroundSF = 2314, Bedroom = 3, FullBath = 2, Fireplaces = 1, GarageSF = 502, ScreenPorchSF = 0)

predict.lm(modAmes2, newx, interval="prediction", level = .95)
```

```{r}
# Our model transformed price by taking the natural log so we have to recalculate the prediction interval
exp(c(5.530761, 5.221046, 5.840476))
```

The 95% prediction interval for this house is (185.1277, 343.9430). When buying a house that fits the relevant characteristics of this house, we are 95% confident that the price of the house will be between 185.1277 thousand dollars and 343.9430 thousand dollars. This doesn't seem unreasonable by just looking at the overall properties of this house.


# Part 6. Cross-validation

## Residual analysis of AmesTest dataset

We now take our final model we developed from the training set and cross-validate it with our testing set. It is important to know whether our model is a good fit or if there is over/underfitting present. We start off with calculating the residuals of the test set.
```{r}
# Compute predicted Price for each of the cases in the test sample, using your model resulting from the initial fit and residual analysis in parts 1 and 2
fitPrice = predict(modAmes, newdata = AmesTest)

# Compute the residuals for the test cases
modAmesTest = lm(formula = Price ~ LotFrontage + LotArea + Quality + Condition + YearBuilt + YearRemodel + BasementSF + GroundSF + BasementFBath + FullBath + Bedroom + TotalRooms + Fireplaces + GarageSF + ScreenPorchSF, data = AmesTest)

holdoutresid = AmesTest$Price - fitPrice
standard = rstandard(modAmesTest)
student = rstudent(modAmesTest)

# Compute the mean and standard deviation of these residuals
mean(holdoutresid)
sd(holdoutresid)
hist(holdoutresid)
```

Based on our training model, the mean of the residuals is expected to be approximately 0. The training data residual mean is -2.892388 x 10^-16 while the test sample has a residual mean of 0.874202. For 200 new data points, this is very close to what we expect the mean to be. Likewise, the training data residual standard deviation is 29.8789 while the testing data has a residual standard deviation of 27.58993. The standard deviations are very close as well between the two data sets. The histogram of the test sample residuals shows a bell-shape centered at 0, and one standard deviation is indeed approximately 25 which supports our numbers. Thus, both the mean and standard deviation of these residuals are close to what we expect using the training model. 

## Outliers

Are there any cases in the test dataset that are especially poorly predicted by the training model? If so, we identify by the row number(s) in the test data and look at the scatterplot for comparison.
```{r}
# Plot of test dataset
plot(rstandard(modAmesTest)~modAmesTest$fitted.values)

# Identifying outliers by row number
m <- abs(standard)>3
(1:200)[m]
```

There are three standardized residuals that have an absolute value greater than 3 at rows 102, 140, 179. In the residuals vs fitted values plot, you can see these three residuals on the plot as well (Two at the top right, one on the bottom). Now we will check with the studentized residuals to see if these test cases are outliers.

## Standarized and studentized residuals

```{r}
# Comparing standarized with studentized residuals
standard[102]
student[102]
standard[140]
student[140]
standard[179]
student[179]

# Cook's Distance plot
cooksplot(modAmesTest)
```

For all three cases, the standardized and studentized residuals are very close in value. This indicates that the fit of the model does not change much without the inclusion of these cases meaning they are not outliers. This is reinforced in our Cook's Distance plot where we see that cases 102, 140, and 179 all lie within the outer red lines. Thus none of the test dataset is significantly poorly predicted by our model.

## Cross-validation calculation

```{r}
# Compute the correlation between the predicted values above and actual prices for the test dataset
cor(AmesTest$Price, fitPrice)
trainingRsq = .8556
crosscorr = cor(AmesTest$Price, fitPrice)
holdoutRsq = crosscorr^2

trainingRsq - holdoutRsq
```

We find the cross-validation correlation between the training and test set. For a model to be a good fit, the shrinkage must be 0 or very close to it. When calculating the shrinkage, we found the shrinkage to be -0.02312466. This means that our testing data fits our model approximately 2% better than our training data. It happened by chance that testing data is a stronger fit for our model.



